{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc42a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import glob \n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import soundfile\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import Audio\n",
    "import sys\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization, Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c57699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import xgboost as xgb\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b858edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d61865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caca72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FUNCTION TO STRETCH THE SOUND\n",
    "def stretch(x, rate=0.8):\n",
    "    data = librosa.effects.time_stretch(x, rate)\n",
    "    return data\n",
    "\n",
    "# FUNCTION TO STRETCH THE SOUND\n",
    "def stretch(x, rate=0.8):\n",
    "    data = librosa.effects.time_stretch(x, rate)\n",
    "    return data\n",
    "\n",
    "# FUNCTION TO ADD WHITE NOISE\n",
    "def noise(x):\n",
    "    noise_amp = 0.05*np.random.uniform()*np.amax(x)   \n",
    "    x = x.astype('float64') + noise_amp * np.random.normal(size=x.shape[0])\n",
    "    return x\n",
    "\n",
    "\n",
    "# FUNCTION FOR pre emphasis filter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "y, sr = librosa.load(librosa.ex('trumpet'))\n",
    "\n",
    "\n",
    "# feature extract function\n",
    "\n",
    "def extract_feature(file_name):\n",
    "    \"\"\"Function Extracts Features from WAV file\"\"\"\n",
    "    orig, sample_rate = librosa.load(file_name)\n",
    "    X = librosa.effects.preemphasis(orig)\n",
    "    stft=np.abs(librosa.stft(X))\n",
    "    result=np.array([])\n",
    "    mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) #Mel-frequency cepstral coefficients (MFCCs)\n",
    "    result=np.hstack((result, mfccs))\n",
    "    chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    result=np.hstack((result, chroma))\n",
    "    mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "    result=np.hstack((result, mel))\n",
    "    return result\n",
    "\n",
    "# dict emotion labels & gender\n",
    "#anger, disgust, fear, happiness, sadness, and surprise\n",
    "\n",
    "\n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}\n",
    "\n",
    "def gender(g):\n",
    "    \"\"\"Returns Gender Label\"\"\"\n",
    "    if int(g[0:2]) % 2 == 0:\n",
    "        return 'female'\n",
    "    else:\n",
    "        return 'male'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edc65b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract name & path from files\n",
    "\n",
    "import os\n",
    "from fnmatch import fnmatch\n",
    "\n",
    "root = \"/Users/davidpitoun/All_Ironhack/Final Project/data/song_speech/\"\n",
    "pattern = \"*.wav\"\n",
    "\n",
    "filenames = []\n",
    "pathx = []\n",
    "path_name = []\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        if fnmatch(name, pattern):\n",
    "            filenames.append(name)\n",
    "            pathx.append(path)\n",
    "            path_name.append(os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cf5c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in path_name:\n",
    "    name = os.path.basename(i)\n",
    "    yoo = name.split(\"-\")[2] \n",
    "    if \"01\" in yoo:\n",
    "        path_name.remove(i)\n",
    "    if \"02\" in yoo:\n",
    "        path_name.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd56b8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2077"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90c6e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "def load_data(test_size=0.2):\n",
    "    \"\"\"Loads Data from directory containing WAV files.\"\"\"\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for file in tqdm(path_name):\n",
    "        file_name=os.path.basename(file)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]] + '_' + gender(file_name.split(\"-\")[-1])\n",
    "        feature=extract_feature(file)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "        le.fit(y)# didnt apply\n",
    "        y2 = le.transform(y)# didnt apply forgot to input y as y2\n",
    "    return train_test_split(np.array(x), y2, test_size=test_size, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a8c5dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2077/2077 [08:45<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb8e4032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1661, 180), (416, 180))\n"
     ]
    }
   ],
   "source": [
    "print((X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f0671c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "print(f'Features extracted: {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afcf513d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.68133057e+02, -6.02271366e+00, -1.21641998e+01, ...,\n",
       "         2.71150111e-06,  2.89187057e-07,  2.80117707e-09],\n",
       "       [-6.18547668e+02,  9.51341152e+00, -4.64498825e+01, ...,\n",
       "         1.40903485e-05,  1.16811498e-06,  8.60882476e-09],\n",
       "       [-6.43316223e+02,  1.53809059e+00, -1.78000431e+01, ...,\n",
       "         6.95666458e-05,  6.56706425e-06,  5.43515846e-08],\n",
       "       ...,\n",
       "       [-4.57340302e+02, -1.87711277e+01, -3.37659225e+01, ...,\n",
       "         4.97421902e-03,  4.93399799e-04,  5.29437921e-06],\n",
       "       [-5.68488647e+02, -3.45060768e+01, -3.07455120e+01, ...,\n",
       "         8.97173362e-04,  7.25102509e-05,  1.47632329e-06],\n",
       "       [-7.06416077e+02, -3.05436859e+01, -3.18417473e+01, ...,\n",
       "         4.03238701e-05,  5.16455839e-06,  1.47998549e-07]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7eb407bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/330 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.4709982788296041\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.48307515777395305\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.48307515777395305\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.48307515777395305\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.4878944348823867\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.4878944348823867\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.5045897877223179\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.5045897877223179\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.5045897877223179\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.5045897877223179\n",
      "\n",
      "Best pipeline: LogisticRegression(MaxAbsScaler(input_matrix), C=5.0, dual=False, penalty=l2)\n"
     ]
    }
   ],
   "source": [
    "# define TPOTClassifier\n",
    "tpotmodel = TPOTClassifier(\n",
    "   generations=10,\n",
    "   population_size=30,\n",
    "   config_dict='TPOT light',\n",
    "   cv=5,\n",
    "   verbosity=2)\n",
    "\n",
    "# performing the search for best fit\n",
    "\n",
    "tpotmodel.fit(X_test, y_test)\n",
    "\n",
    "# exporting best model\n",
    "tpotmodel.export('tpot_spsdataunfilt.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a694003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  7,  9, ...,  9,  6, 12])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpotmodel.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a985700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8341346153846154"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  TPOTClassifier with Speech/Song & preemp,-neutral&calm\n",
    "# logreg or mlp?\n",
    "tpotmodel.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "18d5de66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8341346153846154\n"
     ]
    }
   ],
   "source": [
    "print(tpotmodel.score(X_test, y_test))\n",
    "tpotmodel.export('tpot_top.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8b10e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6354378818737271"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  TPOTClassifier with Speech/Song & preemp,-neutral&calm\n",
    "# 1sttpot with mlp\n",
    "tpotmodel.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d98b553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(random_state=30, max_iter=300,alpha=0.1, learning_rate_init=0.001).fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fe5c7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "44e410b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7355769230769231"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_train1)\n",
    "clf.score(X_test1,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "da6ef101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02403846153846154"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''set back 2nd line to y_test'''\n",
    "y_pred=clf.predict(X_test)\n",
    "clf.score(X_test1,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0c43ca60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  2,  0,  0,  3,  0,  2,  0,  0,  0,  0,  0,  2,  0,  1,  0],\n",
       "       [ 0, 33,  0,  0,  0,  3,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  6,  0,  0,  0,  1,  0,  0,  0,  0,  0,  3,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1,  0,  1,  0,  0,  0,  1,  0,  1,  0,  2,  0,  0],\n",
       "       [ 0,  0,  0,  0, 15,  1,  1,  0,  1,  1,  0,  0,  2,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0,  1, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1],\n",
       "       [ 6,  0,  0,  0,  2,  0, 14,  0,  3,  0,  0,  0, 12,  0,  3,  0],\n",
       "       [ 0,  3,  0,  0,  0,  3,  0, 22,  0,  1,  0,  0,  0,  7,  0,  2],\n",
       "       [ 0,  0,  0,  0,  0,  0,  1,  0, 29,  0,  0,  0,  0,  0,  1,  1],\n",
       "       [ 1,  2,  0,  2,  0,  2,  0,  3,  0, 27,  0,  0,  0,  4,  0,  2],\n",
       "       [ 0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  4,  0,  2,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  5,  0,  5,  0,  0],\n",
       "       [ 1,  0,  0,  0,  2,  0,  9,  0,  1,  0,  1,  0, 32,  0,  2,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  4,  0,  2,  0,  1,  0, 28,  0,  2],\n",
       "       [ 0,  0,  0,  0,  1,  0,  0,  0,  2,  0,  1,  0,  1,  0,  8,  0],\n",
       "       [ 0,  0,  0,  0,  0,  2,  0,  1,  0,  3,  0,  0,  0,  6,  0,  9]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(array([15,  9,  0, 12, 12,  4,  9,  0,  2, 13,  7,  0, 14,  1, 13, 11,  8,\n",
    "        4,  7, 12,  4,  0, 11,  7,  7,  3, 13,  8,  9,  4, 11, 12,  6,  6,\n",
    "        7, 12,  1, 12, 12,  4,  1, 12,  1,  5,  6,  7,  1, 12,  0,  9,  8,\n",
    "        6,  6,  1,  7,  2, 13,  9,  9, 15,  6,  7,  0,  0,  8,  1,  2, 11,\n",
    "        6, 12, 12, 10,  5,  9,  7,  3,  6, 13, 15,  8,  9,  1,  0,  7, 11,\n",
    "       14, 15,  4, 12, 10,  9, 15, 15, 15, 11, 14,  0, 10, 15,  6, 12,  5,\n",
    "        6,  4,  8, 12, 13,  2,  1,  0,  6,  9,  8, 12,  0,  9,  6,  3, 13,\n",
    "       15,  4,  6,  8,  1,  5,  6,  8, 12,  5, 15,  7,  1,  6,  1,  6, 12,\n",
    "       14,  6,  8,  3,  8,  8,  0, 12,  1, 12,  9,  9, 13, 14, 12,  5,  1,\n",
    "        8,  8,  6,  6,  6, 13, 12,  4, 14,  8, 11,  2, 13, 11,  9,  0, 12,\n",
    "        4, 13,  0,  6,  9,  6,  7, 15, 14, 15,  6,  0,  6,  3,  9,  7,  8,\n",
    "        7,  6,  1,  1,  7, 13, 11,  5, 12,  9,  0, 13,  1, 12, 13, 13,  1,\n",
    "        9, 13,  0, 10, 12, 12, 14,  4, 12,  9,  0,  5,  7,  1,  8,  4, 12,\n",
    "        0,  7, 13, 10,  8,  1,  8,  7,  0,  9,  0,  9, 13, 13,  0,  9, 13,\n",
    "        7,  6,  9,  8,  4,  8,  2,  1, 13,  4, 12, 12,  1, 11, 12,  0,  7,\n",
    "       10, 15, 12,  8,  9,  7,  9, 12,  1, 15,  6,  7,  8,  9, 13,  1, 12,\n",
    "        4,  8,  0, 12,  9,  8,  7, 14,  8, 13, 13,  1,  9,  7, 15,  7,  3,\n",
    "        9,  9,  5,  9, 12,  1,  2,  6,  7,  0, 12, 13,  1,  6,  6, 11,  5,\n",
    "        8, 14,  8,  6,  1, 13, 10, 12,  7,  4,  0,  7,  9,  9,  4, 13,  9,\n",
    "        8,  5,  0, 13,  7,  2,  9,  1,  8,  7,  6,  7,  7,  8,  9, 12,  7,\n",
    "       13,  0,  7,  0,  5, 15,  6,  6, 12,  2, 15,  6,  9, 15,  9,  7,  8,\n",
    "        6,  1,  6, 12,  4,  1, 13,  6,  7, 13, 12,  7, 14,  0, 12, 13, 12,\n",
    "        0,  6, 13,  1,  0, 12,  1,  0,  4,  9,  0, 14, 15,  7, 12,  1,  4,\n",
    "       14, 12,  6,  1, 13,  0, 15,  2,  5,  1,  9, 12, 13,  4, 15,  9, 13,\n",
    "        9,  4,  1, 13, 11,  9,  1,  1]), array([15,  9,  0,  4,  6,  4,  9,  0,  2, 13, 13,  1, 14,  1, 13, 13,  8,\n",
    "        4, 15, 12,  4,  0,  5,  7,  7,  3,  7,  8, 13,  4, 13, 12, 14,  8,\n",
    "        7,  6,  9, 12, 12, 14,  1,  6,  1,  5,  6,  7,  1,  6,  0,  9,  8,\n",
    "        6, 12,  1,  5,  2, 13,  9, 13, 15, 14, 15,  0,  0,  8,  1,  2, 11,\n",
    "        6, 12,  6, 10,  5,  9,  7, 11, 12, 13, 13,  8,  9,  1,  0,  9, 13,\n",
    "       14, 13,  4, 12, 12,  1, 15, 15,  9, 11, 14,  0, 10, 15, 12, 12,  5,\n",
    "       14,  4,  8, 12, 13,  2,  5, 14,  0,  7,  8,  6,  0, 13, 12, 13, 13,\n",
    "        9,  6,  6,  8,  1,  5,  4,  8,  8,  5,  9,  5,  1, 12,  1,  6, 12,\n",
    "        8,  8, 14, 13,  8,  8,  0, 12,  1, 12,  5,  9, 13, 14, 12,  5,  1,\n",
    "        8,  8, 12,  0,  6, 13, 12,  8, 14,  8,  7, 12, 13, 11,  9,  0, 12,\n",
    "        5, 13,  6,  0,  9,  6,  7, 15, 12, 13,  6,  4,  6,  9, 13,  7,  8,\n",
    "        5, 12,  1,  1, 13,  9, 13,  5,  6,  9,  0, 13,  1, 12,  7, 13,  5,\n",
    "        9, 13,  0, 12, 12, 12, 10,  4, 12,  9,  0,  5,  7,  1,  8,  4, 12,\n",
    "       12,  7, 11,  2,  8,  1,  8,  7,  1,  9,  0,  9, 13, 13,  0,  9, 13,\n",
    "        7, 12,  9,  8,  4,  8, 12,  1, 13,  4, 12, 12,  1, 11, 12,  0,  7,\n",
    "       10,  5, 14,  8,  7,  1,  9, 12,  1, 13,  6, 13,  6,  3, 13,  1, 12,\n",
    "        4,  8,  4, 12,  9,  8, 13, 14,  8, 13,  7,  9,  5, 13, 15,  7,  5,\n",
    "        9,  9,  5,  3, 12,  1,  2, 12,  7,  6, 12, 13,  1,  0,  8, 13,  4,\n",
    "       15, 14,  8,  0,  1, 13, 10, 10,  7,  4,  0,  7,  9,  0,  4, 13,  9,\n",
    "        8,  5,  0, 13, 13,  2,  9,  1,  8,  1, 12,  1,  7,  8,  9, 12, 13,\n",
    "       13,  0,  7,  0, 15, 13,  6, 12, 12,  6, 15, 12, 15, 15,  9,  7,  8,\n",
    "        6,  1,  6,  6, 12,  1, 13,  0,  7, 13,  0,  7, 14,  0, 12, 15, 14,\n",
    "       12,  4, 15,  1,  4, 12,  1,  0, 12,  7,  0,  4,  7,  7,  6,  1,  9,\n",
    "        8, 12,  6,  5, 13,  0,  5, 12,  5,  1,  9,  4, 13,  4, 13,  1,  9,\n",
    "        9,  4,  1,  7, 11, 15,  9,  1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5d1bab46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3557692307692308"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=30, max_iter=300,alpha=0.1, learning_rate_init=0.01).fit(X_train, y_train)\n",
    "clf.predict(X_train1)\n",
    "clf.score(X_test1,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbcdacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.6333592979176403\tMLPClassifier(StandardScaler(input_matrix), MLPClassifier__alpha=0.1, MLPClassifier__learning_rate_init=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4b5435bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6682692307692307"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=30, max_iter=300,alpha=0.1, learning_rate_init=0.01).fit(X_train1, y_train)\n",
    "clf.predict(X_train1)\n",
    "clf.score(X_test1,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2960663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a standardization\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "X_train1 = scaler.fit_transform(X_train)\n",
    "X_test1= scaler.fit_transform(X_test)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8546e377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6370192307692307"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "exported_pipeline = make_pipeline(\n",
    "    MaxAbsScaler(),\n",
    "    LogisticRegression(C=5.0, dual=False, penalty=\"l2\")\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(X_train1, y_train)\n",
    "exported_pipeline.predict(X_train1)\n",
    "exported_pipeline.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef12d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28a33e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  9,  9, ...,  7,  6, 12])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Average CV score on the training set was: 0.5492181021206018\n",
    "exported_pipeline2 = GradientBoostingClassifier(learning_rate=0.1, max_depth=3, max_features=0.5, min_samples_leaf=2, min_samples_split=11, n_estimators=100, subsample=0.5)\n",
    "# Fix random state in exported estimator\n",
    "if hasattr(exported_pipeline2, 'random_state'):\n",
    "    setattr(exported_pipeline2, 'random_state', 20)\n",
    "\n",
    "exported_pipeline2.fit(X_train1, y_train)\n",
    "exported_pipeline2.predict(X_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ea15adfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31971153846153844"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_pipeline2.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        FunctionTransformer(copy)\n",
    "    ),\n",
    "    MinMaxScaler(),\n",
    "    LogisticRegression(C=20.0, dual=False, penalty=\"l2\")\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(X_train, y_train)\n",
    "exported_pipeline.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "41094be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6057692307692307"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = LogisticRegression(C=20.0, dual=False, penalty=\"l2\")\n",
    "mod.fit(X_train1, y_train)\n",
    "mod.predict(X_train1)\n",
    "mod.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8135fda6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tpot.builtins'; 'tpot' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtpot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tpotClassifier\n",
      "File \u001b[0;32m~/All_Ironhack/Final Project/Code/tpot.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPClassifier\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_pipeline, make_union\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtpot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuiltins\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackingEstimator\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionTransformer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tpot.builtins'; 'tpot' is not a package"
     ]
    }
   ],
   "source": [
    "from tpot import tpotClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4516efc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tpot.builtins'; 'tpot' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackingClassifier\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, TransformerMixin, is_classifier\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtpot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuiltins\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackingEstimator\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n",
      "File \u001b[0;32m~/All_Ironhack/Final Project/Code/tpot.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPClassifier\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_pipeline, make_union\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtpot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuiltins\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackingEstimator\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionTransformer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tpot.builtins'; 'tpot' is not a package"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_union\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import copy\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, is_classifier\n",
    "from tpot.builtins import StackingEstimator\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, min_samples_leaf=11, min_samples_split=7)]\n",
    "\n",
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        StackingClassifier(estimators=estimators)\n",
    "    ),\n",
    "    PCA(iterated_power=9, svd_solver=\"randomized\"),\n",
    "    LogisticRegression(C=0.1, dual=False, penalty=\"l2\")\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(X_train1, y_train)\n",
    "exported_pipeline.predict(X_train1)\n",
    "exported_pipeline.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f396c79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6177884615384616"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=LogisticRegression().fit(X_train1, y_train)\n",
    "m.predict(X_test1)\n",
    "m.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bde89cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('rf', DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, min_samples_leaf=11, min_samples_split=7)),\n",
    "    ('svr', MLPClassifier(random_state=30, max_iter=300,alpha=0.1, learning_rate_init=0.001))]\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "612954eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('rf',\n",
       "                                DecisionTreeClassifier(criterion='entropy',\n",
       "                                                       max_depth=3,\n",
       "                                                       min_samples_leaf=11,\n",
       "                                                       min_samples_split=7)),\n",
       "                               ('svr',\n",
       "                                MLPClassifier(alpha=0.1, max_iter=300,\n",
       "                                              random_state=30))],\n",
       "                   final_estimator=LogisticRegression())"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b61a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.train(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9d5eb756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7307692307692307"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a8fe9c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "disp.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "494f3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu','logistic'],\n",
    "    'solver': ['sgd', 'adam','lbfgs'],\n",
    "    'alpha': [0.0001, 0.01,0.001,0.0015],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fe01f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "24d18030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7067307692307693"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=30, max_iter=300)\n",
    "est = GridSearchCV(mlp, parameters)\n",
    "est.fit(X_train1, y_train)\n",
    "est.predict(X_train1)\n",
    "est.score(X_test1,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridS_ResMlp = pd.DataFrame(est.cv_results_)\n",
    "gridS_ResMlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a1f9c88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.01, max_iter=300, random_state=30)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7511888b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7098086037845073"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "25424b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7067307692307693"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_pipelineB = GradientBoostingClassifier(learning_rate=0.1, max_depth=3, max_features=0.5, min_samples_leaf=2, min_samples_split=11, n_estimators=100, subsample=0.5)\n",
    "# Fix random state in exported estimator\n",
    "if hasattr(exported_pipelineB, 'random_state'):\n",
    "    setattr(exported_pipelineB, 'random_state', 20)\n",
    "\n",
    "exported_pipelineB.fit(X_train1, y_train)\n",
    "exported_pipelineB.predict(X_train1)\n",
    "est.score(X_test1,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60426a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best params for MLP classifier, determined by a grid search\n",
    "model_params = {\n",
    "    'alpha': 0.01,\n",
    "    'batch_size': 256,\n",
    "    'epsilon': 1e-08, \n",
    "    'hidden_layer_sizes': (300,), \n",
    "    'learning_rate': 'adaptive', \n",
    "    'max_iter': 500, \n",
    "}\n",
    "\n",
    "# initialize Multi Layer Perceptron classifier\n",
    "# with best parameters ( so far )\n",
    "model = MLPClassifier(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c935005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d8aa84e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6706730769230769"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Model\n",
    "\n",
    "\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel,random_state=0)\n",
    "gpc.fit(X_train1, y_train)\n",
    "gpc.predict(X_test1)\n",
    "gpc.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b89433e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFwe, chi2\n",
    "from sklearn.feature_selection import SelectFpr, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5696f004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6394230769230769"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    StackingEstimator(estimator=DecisionTreeClassifier(criterion=\"gini\", max_depth=1, min_samples_leaf=11, min_samples_split=19)),\n",
    "    SelectFwe(score_func=f_classif, alpha=0.012),\n",
    "    LogisticRegression(C=0.5, dual=False, penalty=\"l2\")\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(X_train1, y_train)\n",
    "exported_pipeline.predict(X_test1)\n",
    "exported_pipeline.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2af22b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31971153846153844"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_pipeline = GradientBoostingClassifier(learning_rate=0.1, max_depth=3, max_features=0.5, min_samples_leaf=2, min_samples_split=11, n_estimators=100, subsample=0.5)\n",
    "# Fix random state in exported estimator\n",
    "if hasattr(exported_pipeline, 'random_state'):\n",
    "    setattr(exported_pipeline, 'random_state', 20)\n",
    "\n",
    "exported_pipeline.fit(X_train1, y_train)\n",
    "exported_pipeline.predict(X_test1)\n",
    "exported_pipeline.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0279d572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6370192307692307"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    MaxAbsScaler(),\n",
    "    LogisticRegression(C=5.0, dual=False, penalty=\"l2\")\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(X_train1, y_train)\n",
    "exported_pipeline.predict(X_test1)\n",
    "exported_pipeline.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b35ae0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6322115384615384"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        ZeroCount(),\n",
    "        FunctionTransformer(copy)\n",
    "    ),\n",
    "    LogisticRegression(C=0.5, dual=False, penalty=\"l2\")\n",
    ")\n",
    "exported_pipeline.fit(X_train1, y_train)\n",
    "exported_pipeline.predict(X_test1)\n",
    "exported_pipeline.score(X_test1, y_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "556a14e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19903acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        StackingEstimator(estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, min_samples_leaf=11, min_samples_split=7))\n",
    "    ),\n",
    "    PCA(iterated_power=9, svd_solver=\"randomized\"),\n",
    "    LogisticRegression(C=0.1, dual=False, penalty=\"l2\")\n",
    ")\n",
    "exported_pipeline.fit(X_train1, y_train)\n",
    "exported_pipeline.predict(X_test1)\n",
    "exported_pipeline.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e1f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        FunctionTransformer(copy)\n",
    "    ),\n",
    "    MinMaxScaler(),\n",
    "    LogisticRegression(C=20.0, dual=False, penalty=\"l2\")\n",
    ")\n",
    "exported_pipeline.fit(X_train1, y_train)\n",
    "exported_pipeline.predict(X_test1)\n",
    "exported_pipeline.score(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7bb18792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/2100 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.5143144004589788\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.5191910499139415\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.5191910499139415\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.521514629948365\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.521514629948365\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.521514629948365\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.521514629948365\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 16 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 17 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 18 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 19 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Generation 20 - Current best internal CV score: 0.52877223178428\n",
      "\n",
      "Best pipeline: MLPClassifier(MLPClassifier(CombineDFs(input_matrix, CombineDFs(input_matrix, GaussianNB(input_matrix)))))\n"
     ]
    }
   ],
   "source": [
    "# define TPOTClassifier\n",
    "\n",
    "tpot_config = {\n",
    "    'sklearn.naive_bayes.GaussianNB': {\n",
    "    },\n",
    "    'sklearn.ensemble.GradientBoostingClassifier': {\n",
    "    },\n",
    "    'sklearn.linear_model.’':{\n",
    "    }\n",
    "}\n",
    "\n",
    "tpotmodelf = TPOTClassifier(\n",
    "   generations=20,\n",
    "   population_size=100,\n",
    "   #config_dict='TPOT light',\n",
    "   cv=5,\n",
    "   verbosity=2,\n",
    "   config_dict=tpot_config\n",
    ")\n",
    "\n",
    "# performing the search for best fit\n",
    "\n",
    "tpotmodelf.fit(X_test1, y_test)\n",
    "\n",
    "# exporting best model\n",
    "tpotmodelf.export('tpot.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce25cf27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StackingEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m exported_pipeline \u001b[38;5;241m=\u001b[39m make_pipeline(\n\u001b[1;32m      2\u001b[0m     make_union(\n\u001b[1;32m      3\u001b[0m         FunctionTransformer(copy),\n\u001b[1;32m      4\u001b[0m         make_union(\n\u001b[1;32m      5\u001b[0m             FunctionTransformer(copy),\n\u001b[0;32m----> 6\u001b[0m             \u001b[43mStackingEstimator\u001b[49m(estimator\u001b[38;5;241m=\u001b[39mGaussianNB())\n\u001b[1;32m      7\u001b[0m         )\n\u001b[1;32m      8\u001b[0m     ),\n\u001b[1;32m      9\u001b[0m     StackingEstimator(estimator\u001b[38;5;241m=\u001b[39mMLPClassifier()),\n\u001b[1;32m     10\u001b[0m     MLPClassifier()\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m exported_pipeline\u001b[38;5;241m.\u001b[39mfit(X_test1, y_test)\n\u001b[1;32m     14\u001b[0m exported_pipeline\u001b[38;5;241m.\u001b[39mpredict(X_train1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StackingEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        FunctionTransformer(copy),\n",
    "        make_union(\n",
    "            FunctionTransformer(copy),\n",
    "            StackingEstimator(estimator=GaussianNB())\n",
    "        )\n",
    "    ),\n",
    "    StackingEstimator(estimator=MLPClassifier()),\n",
    "    MLPClassifier()\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(X_test1, y_test)\n",
    "exported_pipeline.predict(X_train1)\n",
    "exported_pipeline.score(X_test1,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('rf', DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, min_samples_leaf=11, min_samples_split=7)),\n",
    "    ('svr', MLPClassifier(random_state=30, max_iter=300,alpha=0.1, learning_rate_init=0.001))]\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
